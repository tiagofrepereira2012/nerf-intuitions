"""
The raytracer model defined in 1. is a simple model that can't be used to render complex objects.
Usually people create objects as a mesh of triangles, and the intersection of the rays are computed using this mesh.
However, the diferentiability of a mesh of triangles is not trivial (at least for me), and that's why
the authors of NeRF proposed to use a volumetric rendering approach, and this is what we are going to explore in this script.

"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import torch

def main():
    # Camera parameters
    H = 400
    W = 400
    f = 1200  # Focal length

    # Rays parameters from the origin and direction
    # rays_o: origin of the rays
    # rays_d: direction of the rays
    # Standard here is to stored the rays in a 2D array with shape (H*W, 3)
    rays_o = np.zeros((H * W, 3))
    rays_d = np.zeros((H * W, 3))

    # Pixel coordinates
    u = np.arange(W)
    v = np.arange(H)
    u, v = np.meshgrid(u, v)

    # Seting the directions of the rays, in x, y and z
    dirs = np.stack((u - W / 2, -(v - H / 2), -np.ones_like(u) * f), axis=-1)
    rays_d = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)
    rays_d = rays_d.reshape(-1, 3)

    rays_o = torch.from_numpy(rays_o).float()
    rays_d = torch.from_numpy(rays_d).float()

    # print(rendering(rays_o, rays_d, 0.8, 1.2))
    sphere = SphereVolumetricRendering(
        torch.tensor([0, 0, -1]), 0.1, torch.tensor([1, 0, 0])
    )
    ground_truth = rendering(sphere, rays_o, rays_d, 0.8, 1.2, white_bg_reg=False)


    print("Plotting the sphere generated by the volumetric rendering")
    plot_img(ground_truth.reshape(H, W, 3).numpy())

    print(
        "Now we will do the optimization procedure to find the parameters of the sphere"
    )

    # Here we will optimize the color parameters
    # Our ground-truth are the variables img that was generated by the volumetric rendering

    color_optimized = torch.tensor([0, 1.0, 0], requires_grad=True)
    optimizer = torch.optim.Adam([color_optimized], lr=0.2)

    images = []
    losses = []
    epochs = [] # Yes, I know, not my best moment here
    for epoch in range(90):
        model = SphereVolumetricRendering(
            torch.tensor([0, 0, -1]), 0.1, color_optimized
        )
        Ax = rendering(model, rays_o, rays_d, 0.8, 1.2)

        loss = ((Ax - ground_truth) ** 2).mean()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss {loss.item()}")
            #plot_img(Ax.reshape(H, W, 3).detach().numpy())
            images.append(Ax.reshape(H, W, 3).detach().numpy())
            losses.append(loss.item())   
            epochs.append(epoch)         


    # Plotting a 9x9 grid of the images of each epoch
    fig, axs = plt.subplots(3, 3, figsize=(10, 10))
    for i in range(3):
        for j in range(3):
            axs[i, j].imshow(images[i * 3 + j])
            axs[i, j].set_title(f"Epoch {epochs[i * 3 + j]}, loss: {losses[i * 3 + j]:.2f}")
            axs[i, j].axis("off")
    plt.show()

def plot_img(img):
    # Plotting the sphere
    fig = plt.figure(dpi=200)
    plt.title("Volume Rendering a sphere in 3D space")
    plt.imshow(img)

    plt.show()


class SphereVolumetricRendering:
    """
    I would need to read the Volumetric Rendering paper to understand this properly, but in short,

    the expected color $C(r)$ of a camera ray $r(t) = o + td$ with near and far bounds $t_{n}$ and $t_{f}$ is given by:

    $C(r) = \\int_{t_{n}}^{t_{f}} T(t) \rho(r(t)) c(r(t),d) dt$

    where $T(t)$ is the transmittance along the ray from $t_{n}$ to $t$. This can be interpreted as the probability of the ray
    not being absorbed or scattered along the ray. This is defined as $T(t) = exp(-\\int_{t_{n}}^{t} \\sigma(r(s)) ds)$, where $\\sigma(r)$
    is the volume density of the medium at point $r(t)$.

    The integral above is solved using numerical integration, and the color of the ray is given by the product of the transmittance.

    Hence, being $\\hat{C}(r)$ the numerical approximation of the integral above, the color of the ray is given by:

    $\\hat{C}(r) = \\sum_{i=1}^{N} T_i(1- \text{exp}(-\rho_i\\delta_i))c_i$, where $T_i = \text{exp}(\\sum_{j=1}^{i-1} \rho_j \\delta_j )$.

    In the paper, $\\delta_i = t_{i+1}-t_i$ is the distance between adjacent samples. It is easier to see the $\\hat{C}(r)$
    is differentiable with respect to the parameters of the scene.

    """

    def __init__(self, p, r, c):
        self.p = p
        self.r = r
        self.c = c

    def intersect(self, x, d=None): # d is the direction which will not be used by this class
        """
        In this approach, insted of computing the intersection of the rays with the sphere,
        we will compute only the density of the transmittance of the rays at a given point x.

        If the $x$ is outside the sphere, the density is 0, otherwise it is some density related to
        the color of the sphere.
        """

        x_coords = x[:, 0]
        y_coords = x[:, 1]
        z_coords = x[:, 2]

        is_inside = (x_coords - self.p[0]) ** 2 + (y_coords - self.p[1]) ** 2 + (
            z_coords - self.p[2]
        ) ** 2 <= self.r**2

        num_rays = x.shape[0]
        colors = torch.zeros((num_rays, 3))
        density = torch.zeros((num_rays, 1))

        colors[is_inside] = self.c.float()
        density[is_inside] = 10.0

        return colors, density


def compute_transmittance(betas):
    accumulated_transmittance = torch.cumprod(betas, dim=1)

    # Shift the accumulated transmittance to the right
    # accumulated_transmittance[:, 1:] = accumulated_transmittance[:, :-1]

    # accumulated_transmittance[:, 0] = 1.0
    # return accumulated_transmittance

    return torch.cat(
        (
            torch.ones(
                accumulated_transmittance.shape[0],
                1,
                device=accumulated_transmittance.device,
            ),
            accumulated_transmittance[:, :-1],
        ),
        dim=1,
    )


def rendering(model_object, rays_o, rays_d, tn, tf, nbins=100, device="cpu", white_bg_reg=True):
    """
    Ths function will render the scene by computing the color of the rays.

    For this we will compute the integral of the transmittance of the rays along the ray.
    """

    t = torch.linspace(tn, tf, nbins).to(device)
    delta = torch.cat((t[1:] - t[:-1], torch.tensor([1e-10]).to(device)))

    # [1, nbins, 1]
    # [nb_rays, 1, 3]

    x = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * t.unsqueeze(-1)  # [nb_rays, nb_bins,3]

    # directions = [nb_rays, nb_bins, 3]
    d = rays_d.expand(x.shape[1], x.shape[0], 3).transpose(0, 1).reshape(-1, 3)

    colors, density = model_object.intersect(x.reshape(-1, 3), d)

    colors = colors.reshape(x.shape[0], nbins, 3)  # [nb_rays, nb_bins, 3]
    density = density.reshape(x.shape[0], nbins)  # [nb_rays, nb_bins, 1]

    alpha = 1 - torch.exp(
        -density * delta.unsqueeze(0)
    )  # Equation by the end of page 6 of the paper # [nb_rays, nb_bins, 1]
    T = compute_transmittance(1 - alpha)*alpha  # [nb_rays, nb_bins, 1]
    
    if white_bg_reg:
        c = (T.unsqueeze(-1) * colors).sum(1)
        weight_sum = T.sum(-1) # [nb_rays]
        return c + 1 - weight_sum.unsqueeze(-1)
    else:
    
        c = (T.unsqueeze(-1) * colors).sum(1)  # [nb_rays, 3] reduced the nbins dimension

        return c


if __name__ == "__main__":
    main()
